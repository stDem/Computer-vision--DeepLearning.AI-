{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1ae61c",
   "metadata": {},
   "source": [
    "# Simple Object Detection in Tensorflow\n",
    "This lab will walk through how to use object detection models available in Tensorflow Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import tempfile\n",
    "from six.moves.urllib.request import urlopen\n",
    "from six import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db9c60",
   "metadata": {},
   "source": [
    "### Download the model from Tensorflow Hub\n",
    "Tensorflow Hub is a repository of trained machine learning models which can be reuses in projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bc5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inception resnet version 2\n",
    "module_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
    "\n",
    "# can choose ssd mobilenet version 2 instead\n",
    "#module_handle = \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86878272",
   "metadata": {},
   "source": [
    "### Load the model\n",
    " load the model specified by the module_handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302d0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "2025-08-13 16:06:23.397350: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Pro\n",
      "2025-08-13 16:06:23.397418: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 24.00 GB\n",
      "2025-08-13 16:06:23.397436: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 8.00 GB\n",
      "2025-08-13 16:06:23.397449: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-08-13 16:06:23.397459: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-08-13 16:06:23.956295: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model = hub.load(module_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba49e4f",
   "metadata": {},
   "source": [
    "### Choose the default signature\n",
    "Some models in the Tensorflow hub can be used for different tasks. So each model's documentation should show what signature to use when running the model.\n",
    "\n",
    "If one wants to see if a model has more than one signature then one can do something like print(hub.load(module_handle).signatures.keys()). In my case, the models I use only have the default signature so I don't have to worry about other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d2f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(_SignatureMap({'default': <ConcreteFunction () -> Dict[['detection_scores', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)], ['detection_class_names', TensorSpec(shape=(None, 1), dtype=tf.string, name=None)], ['detection_class_entities', TensorSpec(shape=(None, 1), dtype=tf.string, name=None)], ['detection_boxes', TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)], ['detection_class_labels', TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)]] at 0x32D5B7290>}))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the available signatures for this particular model\n",
    "model.signatures.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76626027",
   "metadata": {},
   "source": [
    "### choose the 'default' signature for object detector.\n",
    "\n",
    "For object detection models, its 'default' signature will accept a batch of image tensors and output a dictionary describing the objects detected, which is what I want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9246f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = model.signatures['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c6ee9c",
   "metadata": {},
   "source": [
    "### download_and_resize_image\n",
    "This function downloads an image specified by a given \"url\", pre-processes it, and then saves it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46cdc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_resize_image(url, new_width=256, new_height=256):\n",
    "    '''\n",
    "    Fetches an image online, resizes it and saves it locally.\n",
    "\n",
    "    Args:\n",
    "        url (string) -- link to the image\n",
    "        new_width (int) -- size in pixels used for resizing the width of the image\n",
    "        new_height (int) -- size in pixels used for resizing the length of the image\n",
    "\n",
    "    Returns:\n",
    "        (string) -- path to the saved image\n",
    "    '''\n",
    "\n",
    "\n",
    "    # create a temporary file ending with \".jpg\"\n",
    "    _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
    "\n",
    "    # opens the given URL\n",
    "    response = urlopen(url)\n",
    "\n",
    "    # reads the image fetched from the URL\n",
    "    image_data = response.read()\n",
    "\n",
    "    # puts the image data in memory buffer\n",
    "    image_data = BytesIO(image_data)\n",
    "\n",
    "    # opens the image\n",
    "    pil_image = Image.open(image_data)\n",
    "\n",
    "    # resizes the image. will crop if aspect ratio is different.\n",
    "    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # converts to the RGB colorspace\n",
    "    pil_image_rgb = pil_image.convert(\"RGB\")\n",
    "\n",
    "    # saves the image to the temporary file created earlier\n",
    "    pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
    "\n",
    "    print(\"Image downloaded to %s.\" % filename)\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b80e11",
   "metadata": {},
   "source": [
    "### Download and preprocess an image\n",
    "Now, using download_and_resize_image I get a sample image online and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52edbb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image downloaded to /var/folders/9k/fjwgdp8s2bd7ywp5b9yssgw00000gn/T/tmpqhdr320w.jpg.\n"
     ]
    }
   ],
   "source": [
    "# can choose a different URL that points to an image of your choice\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/f/fb/20130807_dublin014.JPG\"\n",
    "\n",
    "# download the image and use the original height and width\n",
    "downloaded_image_path = download_and_resize_image(image_url, 3872, 2592)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd4778",
   "metadata": {},
   "source": [
    "### run_detector\n",
    "This function takes in the object detection model detector and the path to a sample image, then use this model to detect objects and display its predicted class categories and detection boxes.\n",
    "\n",
    "run_detector uses load_image to convert the image into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3af7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    '''\n",
    "    Loads a JPEG image and converts it to a tensor.\n",
    "\n",
    "    Args:\n",
    "        path (string) -- path to a locally saved JPEG image\n",
    "\n",
    "    Returns:\n",
    "        (tensor) -- an image tensor\n",
    "    '''\n",
    "\n",
    "    # read the file\n",
    "    img = tf.io.read_file(path)\n",
    "\n",
    "    # convert to a tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def run_detector(detector, path):\n",
    "    '''\n",
    "    Runs inference on a local file using an object detection model.\n",
    "\n",
    "    Args:\n",
    "        detector (model) -- an object detection model loaded from TF Hub\n",
    "        path (string) -- path to an image saved locally\n",
    "    '''\n",
    "\n",
    "    # load an image tensor from a local file path\n",
    "    img = load_img(path)\n",
    "\n",
    "    # add a batch dimension in front of the tensor\n",
    "    converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
    "\n",
    "    # run inference using the model\n",
    "    result = detector(converted_img)\n",
    "\n",
    "    # save the results in a dictionary\n",
    "    result = {key:value.numpy() for key,value in result.items()}\n",
    "\n",
    "    # print results\n",
    "    print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
    "\n",
    "    print(result[\"detection_scores\"])\n",
    "    print(result[\"detection_class_entities\"])\n",
    "    print(result[\"detection_boxes\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d19bbe",
   "metadata": {},
   "source": [
    "### Run inference on the image\n",
    "I run detector by calling the run_detector function. This prints the number of objects found followed by three lists:\n",
    "\n",
    "- The detection scores of each object found (i.e. how confident the model is),\n",
    "- The classes of each object found,\n",
    "- The bounding boxes of each object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb182836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1755094471.283161  483424 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -2484 } dim { size: -2485 } dim { size: -2486 } dim { size: 1088 } } } inputs { dtype: DT_FLOAT shape { dim { size: -105 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -105 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 17 } } device { type: \"CPU\" model: \"0\" frequency: 2400 num_cores: 12 environment { key: \"cpu_instruction_set\" value: \"ARM NEON\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 16384 l2_cache_size: 524288 l3_cache_size: 524288 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -105 } dim { size: 17 } dim { size: 17 } dim { size: 1088 } } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 objects.\n",
      "[0.65321696 0.61050385 0.6015246  0.592552   0.5917802  0.5815478\n",
      " 0.5505323  0.4957591  0.47425103 0.4732213  0.44066203 0.40511316\n",
      " 0.3980361  0.39406416 0.37148595 0.36155915 0.36150882 0.34689036\n",
      " 0.33362255 0.31253242 0.2887808  0.2575848  0.25749162 0.2519611\n",
      " 0.24781859 0.23412514 0.20431808 0.20324862 0.17988189 0.17965089\n",
      " 0.17374797 0.16431478 0.16031054 0.15895325 0.1562039  0.15468822\n",
      " 0.14754567 0.13622591 0.12740251 0.1255574  0.12102627 0.11813003\n",
      " 0.11387341 0.11229258 0.11129044 0.09718847 0.09137164 0.08975992\n",
      " 0.08880346 0.08633563 0.08337443 0.08095174 0.07988743 0.07741397\n",
      " 0.07731989 0.07631022 0.07507869 0.07386067 0.07233311 0.07204021\n",
      " 0.07110172 0.06935565 0.06825575 0.06427943 0.06248567 0.06226338\n",
      " 0.06211052 0.0594006  0.05798876 0.05784502 0.0572553  0.05346744\n",
      " 0.05304386 0.05242869 0.04892577 0.04813002 0.04578408 0.04424202\n",
      " 0.04337543 0.04277803 0.04262635 0.04163408 0.04081724 0.03976008\n",
      " 0.03945999 0.03944283 0.03863989 0.03768988 0.03759553 0.03566732\n",
      " 0.03359349 0.03333263 0.03275891 0.03231593 0.03134312 0.02978743\n",
      " 0.02858377 0.02856154 0.02822334 0.02787921]\n",
      "[b'Person' b'Person' b'Person' b'Person' b'Footwear' b'Person' b'Building'\n",
      " b'Bicycle' b'Window' b'Building' b'Person' b'Wheel' b'Building'\n",
      " b'Building' b'Person' b'Wheel' b'Building' b'Window' b'Window'\n",
      " b'Building' b'Person' b'Van' b'Person' b'Bicycle wheel' b'Person'\n",
      " b'Window' b'Window' b'Bicycle' b'Building' b'Window' b'Window' b'Man'\n",
      " b'Person' b'Person' b'Woman' b'Clothing' b'Bicycle wheel' b'Window'\n",
      " b'Person' b'Window' b'Land vehicle' b'Land vehicle' b'Clothing'\n",
      " b'Bicycle' b'Window' b'House' b'Land vehicle' b'Land vehicle' b'House'\n",
      " b'Man' b'Window' b'Clothing' b'Footwear' b'Person' b'Window' b'Man'\n",
      " b'Man' b'House' b'Person' b'Building' b'Clothing' b'Window' b'Person'\n",
      " b'Jeans' b'Man' b'Furniture' b'Person' b'Person' b'Person'\n",
      " b'Land vehicle' b'Person' b'Window' b'House' b'Woman' b'Window' b'Man'\n",
      " b'Person' b'Man' b'Clothing' b'Bicycle' b'Man' b'Person' b'Window'\n",
      " b'Person' b'Car' b'Man' b'Car' b'Chair' b'House' b'Window' b'Clothing'\n",
      " b'Tire' b'Clothing' b'Window' b'Land vehicle' b'Window' b'Man' b'Window'\n",
      " b'Bus' b'Clothing']\n",
      "[[5.1278782e-01 5.2925885e-01 6.0162258e-01 5.5207765e-01]\n",
      " [5.1963109e-01 6.0151273e-01 6.4617711e-01 6.3462675e-01]\n",
      " [5.0550711e-01 5.0044084e-01 6.0128838e-01 5.2308434e-01]\n",
      " [4.8633158e-01 4.1272956e-01 6.7882979e-01 4.5991975e-01]\n",
      " [8.1519139e-01 9.5612222e-01 8.4270298e-01 9.8714608e-01]\n",
      " [4.9540991e-01 9.2354840e-01 8.3568782e-01 9.9905145e-01]\n",
      " [1.1479238e-02 1.2222341e-02 7.3866981e-01 4.2463291e-01]\n",
      " [5.7767743e-01 3.6645335e-01 7.1277159e-01 4.8337573e-01]\n",
      " [0.0000000e+00 1.1926236e-01 2.2389613e-01 1.8393047e-01]\n",
      " [7.7412114e-02 4.1299814e-01 5.7953990e-01 5.6044620e-01]\n",
      " [5.1381814e-01 7.4803144e-01 5.9199321e-01 7.6661122e-01]\n",
      " [6.3213789e-01 3.5992548e-01 7.0387030e-01 4.1182616e-01]\n",
      " [0.0000000e+00 7.9705197e-01 6.7336839e-01 1.0000000e+00]\n",
      " [1.6023330e-02 6.8486953e-01 5.5876154e-01 8.1116802e-01]\n",
      " [5.0027692e-01 3.7696633e-01 6.3327295e-01 4.1450131e-01]\n",
      " [6.4054024e-01 4.4508937e-01 7.0298356e-01 4.8343769e-01]\n",
      " [0.0000000e+00 2.1905425e-01 6.6040093e-01 4.3326345e-01]\n",
      " [1.9308027e-03 0.0000000e+00 1.3937683e-01 2.6295694e-02]\n",
      " [2.5720405e-03 9.6666849e-01 1.5372872e-01 1.0000000e+00]\n",
      " [5.5722555e-04 1.5205571e-03 7.6521057e-01 2.6997718e-01]\n",
      " [5.0452483e-01 3.6118776e-01 6.3473177e-01 3.8534221e-01]\n",
      " [4.8340583e-01 6.1965084e-01 5.6270576e-01 6.6155612e-01]\n",
      " [4.9806732e-01 3.6457619e-01 6.6123945e-01 4.0497234e-01]\n",
      " [6.3127881e-01 3.6036417e-01 7.0415378e-01 4.1150135e-01]\n",
      " [5.2181387e-01 5.7764757e-01 5.8759993e-01 6.0071886e-01]\n",
      " [2.1956961e-01 3.4874475e-01 3.3837262e-01 3.7707540e-01]\n",
      " [1.2486308e-01 2.5091293e-01 2.7994090e-01 2.8158078e-01]\n",
      " [5.7718605e-01 3.6229670e-01 7.0702082e-01 4.4181094e-01]\n",
      " [2.5747436e-01 5.6756157e-01 5.3110290e-01 6.8772727e-01]\n",
      " [4.2063955e-02 8.7477314e-01 2.5277346e-01 9.1302884e-01]\n",
      " [1.5635104e-01 4.4340116e-01 2.2221322e-01 4.7578609e-01]\n",
      " [5.0196803e-01 9.2148685e-01 8.3640671e-01 1.0000000e+00]\n",
      " [5.2362257e-01 5.7025951e-01 5.8451957e-01 5.9158343e-01]\n",
      " [5.1324642e-01 6.7927641e-01 5.5099452e-01 6.9257993e-01]\n",
      " [5.1912028e-01 5.9998542e-01 6.4637840e-01 6.3403636e-01]\n",
      " [5.2429777e-01 9.2496204e-01 8.1077713e-01 9.9799955e-01]\n",
      " [6.3818729e-01 4.4291818e-01 7.0165384e-01 4.8409772e-01]\n",
      " [3.4219041e-02 3.5557476e-01 1.6225509e-01 3.7492114e-01]\n",
      " [4.8847678e-01 4.5349696e-01 6.2179548e-01 4.7972572e-01]\n",
      " [9.2880888e-04 3.0769908e-01 1.0653340e-01 3.3205968e-01]\n",
      " [4.8300898e-01 6.1990827e-01 5.6477517e-01 6.6069692e-01]\n",
      " [5.8219284e-01 3.6492977e-01 7.1388066e-01 4.8470786e-01]\n",
      " [5.2354771e-01 7.4919933e-01 5.8537811e-01 7.6531756e-01]\n",
      " [6.0915679e-01 4.2670587e-01 7.0516521e-01 4.8708904e-01]\n",
      " [3.5136864e-01 9.7485608e-01 5.5313069e-01 9.9887872e-01]\n",
      " [0.0000000e+00 8.1122327e-01 6.8641078e-01 9.9715137e-01]\n",
      " [5.7629758e-01 3.5746181e-01 7.0481229e-01 4.4027972e-01]\n",
      " [5.6489241e-01 3.6302310e-01 7.0865035e-01 4.1603634e-01]\n",
      " [1.0937551e-02 2.3315532e-02 7.2652298e-01 4.2174774e-01]\n",
      " [4.8468658e-01 4.1068605e-01 6.9468647e-01 4.6309283e-01]\n",
      " [8.0977745e-02 3.8471529e-01 2.0780870e-01 4.1174638e-01]\n",
      " [5.3828442e-01 6.0357368e-01 6.3477612e-01 6.3440859e-01]\n",
      " [6.2984461e-01 6.1497152e-01 6.4493346e-01 6.2538445e-01]\n",
      " [5.0275809e-01 3.8239601e-01 5.9614605e-01 4.1272238e-01]\n",
      " [0.0000000e+00 1.2452303e-02 1.4019351e-01 2.4738219e-02]\n",
      " [5.1444143e-01 7.4779159e-01 5.9198582e-01 7.6682734e-01]\n",
      " [5.0618213e-01 5.0040692e-01 6.0068130e-01 5.2331203e-01]\n",
      " [0.0000000e+00 2.1128355e-01 6.5079409e-01 4.3430078e-01]\n",
      " [4.8945156e-01 4.5439130e-01 5.7234013e-01 4.7647077e-01]\n",
      " [0.0000000e+00 7.0621598e-01 6.1699879e-01 8.6618966e-01]\n",
      " [5.0917292e-01 4.1628119e-01 6.6930449e-01 4.5959872e-01]\n",
      " [4.6517248e-03 8.0309421e-01 1.5985355e-01 8.4039706e-01]\n",
      " [5.2615100e-01 5.6835294e-01 5.7944036e-01 5.8281022e-01]\n",
      " [6.7192483e-01 9.4027770e-01 8.2127601e-01 9.8925078e-01]\n",
      " [5.0277025e-01 3.7388310e-01 6.4699155e-01 4.1297230e-01]\n",
      " [5.7424390e-01 2.6740092e-01 6.5776908e-01 3.2031855e-01]\n",
      " [4.8605675e-01 4.4450879e-01 6.2478882e-01 4.7350335e-01]\n",
      " [5.1724893e-01 7.5696921e-01 5.8851719e-01 7.7146548e-01]\n",
      " [5.2337497e-01 5.5785012e-01 5.7913953e-01 5.7354158e-01]\n",
      " [6.1246103e-01 4.2733246e-01 7.0608026e-01 4.8825186e-01]\n",
      " [5.2412409e-01 5.6155318e-01 5.7838535e-01 5.8047515e-01]\n",
      " [0.0000000e+00 2.4423178e-01 6.0775466e-02 2.9361343e-01]\n",
      " [1.4892121e-02 2.1474108e-03 7.4544191e-01 2.5979075e-01]\n",
      " [4.9323615e-01 9.2395020e-01 8.3711088e-01 9.9775505e-01]\n",
      " [8.3768480e-03 2.4216573e-01 4.9728516e-02 2.8316256e-01]\n",
      " [5.0533491e-01 3.6017528e-01 6.4356101e-01 3.9146179e-01]\n",
      " [5.1309913e-01 5.2379411e-01 6.0050434e-01 5.4296803e-01]\n",
      " [5.2042133e-01 6.0097867e-01 6.4612412e-01 6.3436639e-01]\n",
      " [5.1822484e-01 5.0339556e-01 5.9754860e-01 5.2268386e-01]\n",
      " [5.9419912e-01 3.6132798e-01 7.0546591e-01 4.1585335e-01]\n",
      " [5.1325643e-01 6.7931694e-01 5.5053395e-01 6.9248217e-01]\n",
      " [5.2230269e-01 5.3619546e-01 5.9756500e-01 5.5316323e-01]\n",
      " [4.2987636e-01 8.2870227e-01 5.8992827e-01 8.6432314e-01]\n",
      " [5.0488466e-01 3.8942698e-01 6.1508077e-01 4.1993609e-01]\n",
      " [5.2658856e-01 6.2717688e-01 5.6329978e-01 6.5372890e-01]\n",
      " [5.0130492e-01 3.6418903e-01 6.5996474e-01 4.0379328e-01]\n",
      " [5.1517117e-01 6.2410480e-01 5.6379533e-01 6.5800208e-01]\n",
      " [5.7313794e-01 2.6690266e-01 6.6616201e-01 3.1864023e-01]\n",
      " [8.3423540e-02 4.0741438e-01 5.8409244e-01 5.5852288e-01]\n",
      " [2.8819689e-01 4.7798458e-04 4.1436461e-01 3.6599576e-02]\n",
      " [4.9727285e-01 4.5529667e-01 5.8381712e-01 4.7793603e-01]\n",
      " [6.2716800e-01 3.6102405e-01 7.0599681e-01 4.0978017e-01]\n",
      " [5.1586121e-01 3.8005701e-01 5.9689385e-01 4.1175830e-01]\n",
      " [1.1809801e-02 3.0812180e-01 9.7285956e-02 3.2503897e-01]\n",
      " [5.1250172e-01 6.2365335e-01 5.6242210e-01 6.5764189e-01]\n",
      " [4.0100315e-01 8.8508880e-01 5.8128160e-01 9.3921447e-01]\n",
      " [5.1385325e-01 5.2948457e-01 6.0200971e-01 5.5236280e-01]\n",
      " [0.0000000e+00 1.0060613e-02 1.3615684e-01 3.1600725e-02]\n",
      " [4.8042628e-01 6.2042278e-01 5.6528461e-01 6.6015041e-01]\n",
      " [5.1935548e-01 3.6184040e-01 6.2499541e-01 3.8491967e-01]]\n"
     ]
    }
   ],
   "source": [
    "run_detector(detector, downloaded_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
